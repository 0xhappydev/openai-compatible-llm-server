# Hey Look, It's an LLM

OpenAI and Ollama compatible API server for local LLM inference with MLX, llama.cpp, and CoreML.

A lightweight API server for running Apple MLX models, GGUF models, and CoreML STT behind OpenAI/Ollama compatible endpoints, with on-the-fly model swapping and optional analytics.

**Platform Support**
- macOS: All backends (MLX, llama.cpp, CoreML STT)
- Linux: llama.cpp backend
- Windows: llama.cpp backend (CUDA, Vulkan, CPU)
